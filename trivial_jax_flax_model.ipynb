{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7240e2df-decc-4cc8-b5a8-ba0565c00891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "from time import time\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  \"\"\"A simple MLP model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(features=20)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=20)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=2)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, X, labels):\n",
    "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = MLP().apply({'params': params}, X)\n",
    "    one_hot = jax.nn.one_hot(labels, 2)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "    return loss, logits\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  return grads, loss, accuracy\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "  train_ds_size = len(train_ds['X'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, len(train_ds['X']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  epoch_loss = []\n",
    "  epoch_accuracy = []\n",
    "\n",
    "  for perm in perms:\n",
    "    batch_X = train_ds['X'][perm, ...]\n",
    "    batch_labels = train_ds['labels'][perm, ...]\n",
    "    grads, loss, accuracy = apply_model(state, batch_X, batch_labels)\n",
    "    # state is a flax TrainState object with a tx property defining step\n",
    "    # tx (effectively the optimizer) is set in create_train_state function below\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    epoch_loss.append(loss)\n",
    "    epoch_accuracy.append(accuracy)\n",
    "  train_loss = np.mean(epoch_loss)\n",
    "  train_accuracy = np.mean(epoch_accuracy)\n",
    "  return state, train_loss, train_accuracy\n",
    "\n",
    "@jax.jit\n",
    "def apply_gradients(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "def create_train_state(rng, config):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  mlp = MLP()\n",
    "  params = mlp.init(rng, jnp.ones([1, 2]))['params']\n",
    "  tx = optax.sgd(config.learning_rate, config.momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=mlp.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def train_and_evaluate(config: ml_collections.ConfigDict,\n",
    "                       ) -> train_state.TrainState:\n",
    "  \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Args:\n",
    "    config: Hyperparameter configuration for training and evaluation.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "  train_ds, test_ds = get_datasets()\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "\n",
    "  rng, init_rng = jax.random.split(rng)\n",
    "  state = create_train_state(init_rng, config)\n",
    "\n",
    "  for epoch in range(1, config.num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, train_loss, train_accuracy = train_epoch(state, train_ds,\n",
    "                                                    config.batch_size,\n",
    "                                                    input_rng)\n",
    "    _, test_loss, test_accuracy = apply_model(state, test_ds['X'],\n",
    "                                              test_ds['labels'])\n",
    "\n",
    "    print(\n",
    "        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "        % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "           test_accuracy * 100))\n",
    "\n",
    "  return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cccd87c-5ea0-46db-bab6-62e5d1562083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, train_loss: 0.1268, train_accuracy: 98.47, test_loss: 0.0459, test_accuracy: 98.61\n",
      "epoch:  2, train_loss: 0.0357, train_accuracy: 99.27, test_loss: 0.0323, test_accuracy: 98.69\n",
      "epoch:  3, train_loss: 0.0277, train_accuracy: 99.25, test_loss: 0.0298, test_accuracy: 98.61\n",
      "epoch:  4, train_loss: 0.0227, train_accuracy: 99.39, test_loss: 0.0330, test_accuracy: 98.27\n",
      "epoch:  5, train_loss: 0.0205, train_accuracy: 99.34, test_loss: 0.0175, test_accuracy: 99.47\n",
      "Total training time: 6\n",
      "Error rate: 0.0053\n"
     ]
    }
   ],
   "source": [
    "def get_datasets():\n",
    "  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "  X = np.random.random((70_000, 2))\n",
    "  labels = (X[:, 0] > X[:, 1]).astype(int)\n",
    "  train_ds = {\n",
    "      'X': X[:60_000],\n",
    "      'labels': labels[:60_000],\n",
    "  }\n",
    "  test_ds = {\n",
    "      'X': X[60_000:],\n",
    "      'labels': labels[60_000:]\n",
    "  }\n",
    "  return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = get_datasets()\n",
    "\n",
    "config = ml_collections.ConfigDict({\n",
    "    'learning_rate': 0.1,\n",
    "    'momentum': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5,\n",
    "})\n",
    "\n",
    "start = time()\n",
    "state = train_and_evaluate(config)\n",
    "print(f'Total training time: {int(time() - start)}')\n",
    "# Find all mistakes in testset.\n",
    "logits = MLP().apply({'params': state.params}, test_ds['X'])\n",
    "error_idxs, = jnp.where(test_ds['labels'] != logits.argmax(axis=1))\n",
    "print(f'Error rate: {len(error_idxs) / len(logits)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e8b67-1a3c-49a7-9042-e194848f69d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
