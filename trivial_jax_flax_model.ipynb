{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da8fe7a-6bda-4e10-b77c-aada2b9bcbdf",
   "metadata": {},
   "source": [
    "My first model with Flax.\n",
    "\n",
    "I created dummy data which is two numeric features. The target is binary classification, where the label is true iff the first number comes before the second when the two numbers are spelled out in alphabetical order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7240e2df-decc-4cc8-b5a8-ba0565c00891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, train_loss: 0.6741, train_accuracy: 59.08, test_loss: 0.5611, test_accuracy: 68.10\n",
      "epoch:  2, train_loss: 0.5626, train_accuracy: 66.97, test_loss: 0.5271, test_accuracy: 71.40\n",
      "epoch:  3, train_loss: 0.5514, train_accuracy: 67.34, test_loss: 0.5113, test_accuracy: 72.10\n",
      "epoch:  4, train_loss: 0.5410, train_accuracy: 68.65, test_loss: 0.5119, test_accuracy: 71.60\n",
      "epoch:  5, train_loss: 0.5343, train_accuracy: 68.70, test_loss: 0.5099, test_accuracy: 71.40\n",
      "epoch:  6, train_loss: 0.5310, train_accuracy: 68.89, test_loss: 0.4993, test_accuracy: 71.00\n",
      "epoch:  7, train_loss: 0.5268, train_accuracy: 69.08, test_loss: 0.5027, test_accuracy: 69.40\n",
      "epoch:  8, train_loss: 0.5255, train_accuracy: 68.76, test_loss: 0.5022, test_accuracy: 71.10\n",
      "epoch:  9, train_loss: 0.5200, train_accuracy: 69.26, test_loss: 0.4852, test_accuracy: 72.60\n",
      "epoch: 10, train_loss: 0.5291, train_accuracy: 68.38, test_loss: 0.4920, test_accuracy: 70.80\n",
      "Total training time: 84\n",
      "Error rate: 0.307\n"
     ]
    }
   ],
   "source": [
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from num2words import num2words\n",
    "from time import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple MLP model.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Since 4 and 400 are close in alphabetical order (even if not in numerical value), I hypothesize that\n",
    "        # the first digit is especially important. So I create a first_digit feature below\n",
    "        num_digits = jnp.log10(jnp.abs(x))\n",
    "        first_digit = jnp.floor((jnp.abs(x) // (10 ** (num_digits)))).clip(0)\n",
    "        x = jnp.hstack([x, num_digits, first_digit])\n",
    "        x = nn.Dense(features=20)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=20)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=2)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, X, labels):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = MLP().apply({\"params\": params}, X)\n",
    "        one_hot = jax.nn.one_hot(labels, 2)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "    train_ds_size = len(train_ds[\"X\"])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, len(train_ds[\"X\"]))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_X = train_ds[\"X\"][perm, ...]\n",
    "        batch_labels = train_ds[\"labels\"][perm, ...]\n",
    "        grads, loss, accuracy = apply_model(state, batch_X, batch_labels)\n",
    "        # state is a flax TrainState object with a tx property defining step\n",
    "        # tx (effectively the optimizer) is set in create_train_state function below\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_gradients(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "def create_train_state(rng, config):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    mlp = MLP()\n",
    "    params = mlp.init(rng, jnp.ones([1, 2]))[\"params\"]\n",
    "    tx = optax.adabelief(config.learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=mlp.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def train_and_evaluate(config: ml_collections.ConfigDict,) -> train_state.TrainState:\n",
    "    \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Args:\n",
    "    config: Hyperparameter configuration for training and evaluation.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "    train_ds, test_ds = make_data()\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config)\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy = train_epoch(\n",
    "            state, train_ds, config.batch_size, input_rng\n",
    "        )\n",
    "        _, test_loss, test_accuracy = apply_model(\n",
    "            state, test_ds[\"X\"], test_ds[\"labels\"]\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f\"\n",
    "            % (epoch, train_loss, train_accuracy * 100, test_loss, test_accuracy * 100)\n",
    "        )\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def make_data(train_size=50_000, val_size=1_000):\n",
    "    total_size = train_size + val_size\n",
    "    X = 10 * np.random.randn(total_size, 2)\n",
    "    X_spelled_out = np.vectorize(num2words)(X)\n",
    "    labels = (X_spelled_out[:, 0] > X_spelled_out[:, 1]).astype(int)\n",
    "    labels = (\n",
    "        np.array([num2words(i) for i in X[:, 0]])\n",
    "        > np.array([num2words(i) for i in X[:, 1]])\n",
    "    ).astype(int)\n",
    "    train_ds = {\n",
    "        \"X\": X[:train_size],\n",
    "        \"labels\": labels[:train_size],\n",
    "    }\n",
    "    test_ds = {\"X\": X[train_size:], \"labels\": labels[train_size:]}\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "train_ds, test_ds = make_data()\n",
    "\n",
    "config = ml_collections.ConfigDict(\n",
    "    {\"learning_rate\": 0.05, \"batch_size\": 256, \"num_epochs\": 10,}\n",
    ")\n",
    "\n",
    "start = time()\n",
    "state = train_and_evaluate(config)\n",
    "print(f\"Total training time: {int(time() - start)}\")\n",
    "# Print final test accuracy\n",
    "logits = MLP().apply({\"params\": state.params}, test_ds[\"X\"])\n",
    "(error_idxs,) = jnp.where(test_ds[\"labels\"] != logits.argmax(axis=1))\n",
    "print(f\"Error rate: {len(error_idxs) / len(logits)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
